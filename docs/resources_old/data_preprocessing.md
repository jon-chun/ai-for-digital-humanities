# Datasets: Preprocessing

This is a list of many of the most common, useful and interesting resources for Computational Digital Humanities. All items are organized into an imperfect taxonomy according to application category.

If you have a resource we should add, please either (a) email us at chunj@kenyon.edu or (b) submit a pull request for this ai-for-the-digital-humanities github repo.


* [Distillation](https://github.com/Guang000/Awesome-Dataset-Distillation)

Dataset distillation is the task of synethesizing a small dataset such that models trained on it achieve high performance on the original large dataset. A dataset distillation algorithm takes as input a large real dataset to be distilled (training set), and outputs a small synthetic distilled dataset, which is evaluated by testing models trained on this distilled dataset on a separate real dataset (validation/test set). A good small distilled dataset is not only useful in dataset understanding, but has various applications (e.g., continual learning and neural architecture search). This task was first introduced in the paper Dataset Distillation [Tongzhou Wang et al., '18], which is presented in details on the project [Dataset Distillation].